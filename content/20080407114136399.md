+++
title = "Linux Clusters 3/3 - El clúster Ubuntu - 2da. parte"
slug = "20080407114136399"
date = "2008-04-07T11:41:00-06:00"
[taxonomies]
tema = ["articulos"]
autor = ["Juan Caballero"]
+++

Esta es la segunda parte del montaje de un clúster con Linux Ubuntu,
ahora veremos la instalación y configuración de los nodos esclavos.

English version:
http://linxe-eye.blogspot.com/2008/04/ubuntu-cluster-slave-nodes.html

<!-- more -->
En el nodo maestro hemos configurado un servidor DHCP que servirá para
asignar las IP, nombre de host y pasar un sistema de arranque por red
mediante PXE. Por lo que ahora solo necesitamos conectar el nodo,
agregarle un teclado y monitor y encenderlo, activando el arranque por
red en el BIOS, e instalar el sistema base.

Para el particionamiento usamos la opción por defecto de usar todo el
disco duro como una sola partición y el swap que asigna, creamos la
cuenta general de acceso (*beagle*) y al finalizar y reiniciar, entramos
y en una terminal cambiamos a *root*, cambiamos su contraseña e
instalamos el servidor SSH:

    sudo  su -passwdapt-get install openssh-server

Repetimos este proceso para todos los nodos y los siguientes pasos se
pueden realizar desde el nodo maestro.

**Acceso automático con SSH**
Necesitamos crear un par de llaves para cada usuario y así puedan
acceder a cualquier nodo sin contraseña, primero creamos la del usuario
general:

    ssh-keygencp .ssh/id_pub .ssh/authorized_keys

*Root* también requiere de sus propias llaves de acceso, pero como no
exportamos su HOME, necesitamos copiarlas a cada unos de los nodos con
rcp. Este paso va a solicitar varias veces la contraseña, pero será
única vez que lo hagamos.

    su -ssh-keygencp .ssh/id_pub .ssh/authorized_keysfor NODE in `cat /etc/machines`do    rsh $NODE mkdir .ssh    rcp .ssh/authorized_keys $NODE:.ssh/authorized_keysdone

En los siguientes pasos se requiere que se acceda a cada nodo como
*root*.

**Exportando HOME**Nos conectamos al nodo, instalamos el paquete NFS,
editamos /etc/fstab para montar /home desde el nodo maestro, borramos el
contenido del viejo HOME y montamos:

    ssh nodeXXapt-get install nfs-commonecho "197.1.1.1:/home /home nfs defaults,auto 0 0" >> /etc/fstabrm -rf /home/*mount -a

Ajustes de hostsEditamos /etc/hosts para incluir todos los nodos,
agregamos:

    197.1.1.1      beagle.local      beagle197.1.1.100    node00.local      node00197.1.1.101    node01.local      node01197.1.1.102    node02.local      node02197.1.1.103    node03.local      node03197.1.1.104    node04.local      node04197.1.1.105    node05.local      node05197.1.1.106    node06.local      node06197.1.1.107    node07.local      node07197.1.1.108    node08.local      node08197.1.1.109    node09.local      node09

**Instalar SGE**
Los archivos necesario son exportados en /home/sgeadmin, primero
añadimos las dependencias, agregamos el usuaro *sgeadmin* e instalamos
el cliente:

    apt-get install binutilsadduser sgeadmin/home/sgeadmin/install_execd

Nota: Hay que revisar que los valores de UID y GID coincidan en
/etc/passwd y /etc/groups, debes er el mismo en todos los nodos y el
maestro.

**Administrando los nodos**
Muchas tareas administrativas son iguales para cada nodo, por lo que
usamos un *script* (/sbin/cluster-fork) para que lo ejecute en cada
nodo:

    #!/bin/bash# cluster-fork COMMANDS# Script to execute COMMANDS in all nodes in /etc/machines# Juan Caballero @ Cinvestav 2008for NODE in `cat /etc/machines`do    echo $NODE:    rsh $NODE $*done

Ahora podemos ejecitar el mismo comando sin problemas, pero es
recomendable ejecutarlos con las opciones no interactivas, por ejemplo
para actualizar todos los nodos:

    cluster-fork apt-get -y updatecluster-fork apt-get -y upgrade

O para instalar ganglia y soporte MPI:

    cluster-fork apt-get -y install ganglia-monitor lam-runtime mpich-bin openmpi-bin

**Add users in the cluster**
El HOME de todo usuario agregado en el nodo maestro será exportado a los
otros nodos, podemos usar adduser en cada nodo para darlo de alta pero
hay que tener cuidado de tener el mismo UID y GID y no crear un /home,
de ser necesario se debe editar los archivos /etc/passwd y /etc/groups,
también no olvidar crearle sus propias llaves de acceso automático.

Finalmente ya tenemos un clúster HPC corriendo en Linux Ubuntu, pero
muchos pasos pueden ser fácilmente adaptados a su distro preferida.
Ahora vamos a probar un poco el desempeño del equipo y probablemente
después coloquemos algunas fotos. También cualquier comentario o
sugerencia son bienvenidos.



Autor: Juan Caballero (linxe (arroba) glib . org . mx)

